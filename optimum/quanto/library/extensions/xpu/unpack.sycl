// Copyright 2024 The HuggingFace Team. All rights reserved.
// Copyright 2024 Intel Corporation. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include <torch/extension.h>
#include <sycl/sycl.hpp>
#include <c10/xpu/XPUStream.h>


inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}
#define BLOCK_SIZE 256

using namespace at;


static torch::Tensor allocate_output(const torch::Tensor& input, int bits) {
    int n_packed = 8 / bits;
    auto output_shape = input.sizes().vec();
    output_shape[0] = output_shape[0] * n_packed;
    return torch::empty(output_shape, input.options());
}

void unpack_4bit_kernel(unsigned char* input, unsigned char* output, int n,
                        const sycl::nd_item<3> &item_ct1) {
    int i = item_ct1.get_group(2) * item_ct1.get_local_range(2) +
            item_ct1.get_local_id(2);
    if (i>=n) return;

    output[i]     = (input[i] & 0x0F);
    output[i + n] = (input[i] & 0xF0) >> 4;
}

class Unpack4BitKrn {
public:
    void operator()(sycl::nd_item<3> item_ct1) const {
        unpack_4bit_kernel(ct0, ct1, numel, item_ct1);
    }
    Unpack4BitKrn(unsigned char* _ct0, unsigned char* _ct1, int64_t _numel):
        ct0(_ct0),
        ct1(_ct1),
        numel(_numel)
    {}
private:
    unsigned char* ct0;
    unsigned char* ct1;
    int64_t numel;
};

static torch::Tensor unpack_4bit(const torch::Tensor& input){
    auto output = allocate_output(input, 4);

    const auto numel = input.numel();
    int blocks = cdiv(numel, BLOCK_SIZE);

    sycl::queue& queue = c10::xpu::getCurrentXPUStream().queue();

    auto krn = [&](sycl::handler &cgh) {
        auto input_data_ptr_unsigned_char_ct0 =
            input.data_ptr<unsigned char>();
        auto output_data_ptr_unsigned_char_ct1 =
            output.data_ptr<unsigned char>();

	Unpack4BitKrn krn2(input_data_ptr_unsigned_char_ct0, output_data_ptr_unsigned_char_ct1, numel);

        cgh.parallel_for<Unpack4BitKrn>(
            sycl::nd_range<3>(
		sycl::range<3>(1, 1, blocks) * sycl::range<3>(1, 1, BLOCK_SIZE),
                sycl::range<3>(1, 1, BLOCK_SIZE)),
	    krn2);
    };
    queue.submit(krn);
    return output;
}

void unpack_2bit_kernel(unsigned char* input, unsigned char* output, int n,
                        const sycl::nd_item<3> &item_ct1) {
    int i = item_ct1.get_group(2) * item_ct1.get_local_range(2) +
            item_ct1.get_local_id(2);
    if (i>=n) return;

    output[i]       = (input[i] & 0x03);
    output[i + n]   = (input[i] & 0x0C) >> 2;
    output[i + n*2] = (input[i] & 0x30) >> 4;
    output[i + n*3] = (input[i] & 0xC0) >> 6;
}

class Unpack2BitKrn {
public:
    void operator()(sycl::nd_item<3> item_ct1) const {
        unpack_2bit_kernel(ct0, ct1, numel, item_ct1);
    }
    Unpack2BitKrn(unsigned char* _ct0, unsigned char* _ct1, int64_t _numel):
        ct0(_ct0),
        ct1(_ct1),
        numel(_numel)
    {}
private:
    unsigned char* ct0;
    unsigned char* ct1;
    int64_t numel;
};

static torch::Tensor unpack_2bit(const torch::Tensor& input){
    auto output = allocate_output(input, 2);

    const auto numel = input.numel();
    int blocks = cdiv(numel, BLOCK_SIZE);
    sycl::queue& queue = c10::xpu::getCurrentXPUStream().queue();
    auto krn = [&](sycl::handler &cgh) {
        auto input_data_ptr_unsigned_char_ct0 =
            input.data_ptr<unsigned char>();
        auto output_data_ptr_unsigned_char_ct1 =
            output.data_ptr<unsigned char>();

	Unpack2BitKrn krn2(input_data_ptr_unsigned_char_ct0, output_data_ptr_unsigned_char_ct1, numel);

        cgh.parallel_for<Unpack2BitKrn>(
            sycl::nd_range<3>(
		sycl::range<3>(1, 1, blocks) * sycl::range<3>(1, 1, BLOCK_SIZE),
                sycl::range<3>(1, 1, BLOCK_SIZE)),
	    krn2);
    };
    queue.submit(krn);
    return output;
}

torch::Tensor unpack(torch::Tensor &t, int bits) {
    TORCH_CHECK(t.scalar_type() == torch::kUInt8, "Unsupported data type: ", t.scalar_type());
    TORCH_CHECK(t.device().is_xpu(), "t must be a XPU  tensor.");
    TORCH_CHECK(t.is_contiguous(), "t must be contiguous.");
    switch(bits) {
      case 4:
        return unpack_4bit(t);
      case 2:
        return unpack_2bit(t);
      default:
        throw std::invalid_argument("Can only unpack 2-bit or 4-bit tensors.");
    }
}
